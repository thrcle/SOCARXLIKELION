{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SOCAR_HW.1ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOygVQ+D/l72Q8RbmKCsl89",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thrcle/SOCARXLIKELION/blob/main/SOCAR_HW_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 데이터셋의 크기가 1/10일 때 , k-fold crossvalidation, gradient descent를 이용한 linear regression\n",
        "### 2) 데이터셋의 크기가 원래 사이즈일때, k-fold crossvalidation, gradient descent를 이용한 linear regression \n",
        "### 3) 1과 2의 성능비교"
      ],
      "metadata": {
        "id": "FZzjICI4iW2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkN8IM3whtPp",
        "outputId": "b35bfc3f-d54c-4c78-ef89-76db0898e6ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i1MEMj73hg9P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 불러오기 \n",
        "file = open('/content/drive/MyDrive/Lion/data/regression_data.txt','r')  # open the file with read-only\n",
        "text = file.readlines()  # read all line texts\n",
        "file.close()  # close the file\n",
        "\n",
        "# 1/10\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "# original\n",
        "x_origin_data = []\n",
        "y_origin_data = []\n",
        "\n",
        "# 데이터 1/10\n",
        "\n",
        "# convert to float\n",
        "for idx,s in enumerate(text):\n",
        "    data = s.split()\n",
        "    x_origin_data.append(float(data[0]))\n",
        "    y_origin_data.append(float(data[1])) \n",
        "    \n",
        "    if idx%10==0:\n",
        "      x_data.append(float(data[0]))\n",
        "      y_data.append(float(data[1]))    \n",
        "\n",
        "# convert to numpy-array\n",
        "x_data = np.asarray(x_data, dtype=np.float32)\n",
        "y_data = np.asarray(y_data, dtype=np.float32)\n",
        "x_origin_data = np.asarray(x_origin_data, dtype=np.float32)\n",
        "y_origin_data = np.asarray(y_origin_data, dtype=np.float32)"
      ],
      "metadata": {
        "id": "cZGZM8C_h6LF"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_origin_data), len(x_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1EGGP8vxvCp",
        "outputId": "e87ea353-1090-40ff-b255-b5e056e00c55"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data shaping for pytorch : (input.shape: (데이터의 갯수, 차원) )\n",
        "if len(x_data.shape)==1 and len(y_data.shape)==1:\n",
        "  x_data = np.expand_dims(x_data, axis=-1)\n",
        "  y_data = np.expand_dims(y_data, axis=-1)\n",
        "\n",
        "if len(x_origin_data.shape)==1 and len(y_origin_data.shape)==1:\n",
        "  x_origin_data = np.expand_dims(x_origin_data, axis=-1)\n",
        "  y_origin_data = np.expand_dims(y_origin_data, axis=-1)\n",
        "print(x_origin_data.shape, y_origin_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY9YZl4ZjxlN",
        "outputId": "2f3e4d81-a924-403d-9231-238cfc6de352"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 1) (100, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# k=np.setdiff1d(x_data,x_data[2:4] )\n",
        "# k2=torch.from_numpy(k)\n",
        "# k2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7xEgKDLpX6V",
        "outputId": "805d7ff6-a029-4c71-de69-0b5577f653f8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x_data[2:4,].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgUiGRZitGY0",
        "outputId": "c9257463-09eb-4492-ecf8-cf5f147f4ae8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "성능비교"
      ],
      "metadata": {
        "id": "PymvhOai0xVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1/10 data\n",
        "# Hyper-parameters\n",
        "input_size = 1\n",
        "output_size = 1\n",
        "num_epochs = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "k=5\n",
        "validation_x_loss=[]\n",
        "\n",
        "val_losses = []\n",
        "\n",
        "for i in range(k):\n",
        "  model = nn.Linear(input_size, output_size) \n",
        "\n",
        "  # Loss and optimizer\n",
        "  criterion = nn.MSELoss()  \n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "  x_valid=x_data[2*i:2*(i+1),]\n",
        "  y_valid=y_data[2*i:2*(i+1),]\n",
        "  \n",
        "  x_train=np.setdiff1d(x_data,x_valid )\n",
        "  y_train=np.setdiff1d(y_data,y_valid )\n",
        "\n",
        "  if len(x_train.shape)==1 and len(y_train.shape)==1:\n",
        "    x_train = np.expand_dims(x_train, axis=-1)\n",
        "    y_train = np.expand_dims(y_train, axis=-1)\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "  for epoch in range(num_epochs):\n",
        "      # Convert numpy arrays to torch tensors\n",
        "      inputs = torch.from_numpy(x_train)\n",
        "      targets = torch.from_numpy(y_train)\n",
        "\n",
        "      # Predict outputs with the linear model.\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets)\n",
        "      \n",
        "      # compute gradients and update\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      if (epoch+1) % 20 == 0:\n",
        "          print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "  # validation error\n",
        "  inputs = torch.from_numpy(x_valid)\n",
        "  targets = torch.from_numpy(y_valid)\n",
        "  outputs = model(inputs)\n",
        "  loss = criterion(outputs, targets)\n",
        "  print(i+1,\"-th round validation error: \",loss.item())\n",
        "  val_losses.append(loss.item())\n",
        "\n",
        "val_losses = np.asarray(val_losses)\n",
        "print(\"Final validation error: \", val_losses.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCR-QZb_kI7G",
        "outputId": "985476fe-4711-41d2-aa28-b55982bffc84"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/100], Loss: 0.0204\n",
            "Epoch [40/100], Loss: 0.0013\n",
            "Epoch [60/100], Loss: 0.0003\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "1 -th round validation error:  8.724057988729328e-05\n",
            "Epoch [20/100], Loss: 0.0132\n",
            "Epoch [40/100], Loss: 0.0017\n",
            "Epoch [60/100], Loss: 0.0004\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "2 -th round validation error:  1.3064389349892735e-05\n",
            "Epoch [20/100], Loss: 0.0059\n",
            "Epoch [40/100], Loss: 0.0006\n",
            "Epoch [60/100], Loss: 0.0002\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "3 -th round validation error:  0.00043226982234045863\n",
            "Epoch [20/100], Loss: 0.0165\n",
            "Epoch [40/100], Loss: 0.0018\n",
            "Epoch [60/100], Loss: 0.0003\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "4 -th round validation error:  0.0003998020547442138\n",
            "Epoch [20/100], Loss: 0.0002\n",
            "Epoch [40/100], Loss: 0.0001\n",
            "Epoch [60/100], Loss: 0.0001\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "5 -th round validation error:  0.0007297149859368801\n",
            "Final validation error:  0.0003324183664517477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# origin data\n",
        "\n",
        "# Hyper-parameters\n",
        "input_size = 1\n",
        "output_size = 1\n",
        "num_epochs = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "k=5\n",
        "validation_x_loss=[]\n",
        "\n",
        "val_losses = []\n",
        "\n",
        "for i in range(k):\n",
        "  model = nn.Linear(input_size, output_size) \n",
        "\n",
        "  # Loss and optimizer\n",
        "  criterion = nn.MSELoss()  \n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "  x_valid=x_origin_data[2*i:2*(i+1),]\n",
        "  y_valid=y_origin_data[2*i:2*(i+1),]\n",
        "  \n",
        "  x_train=np.setdiff1d(x_origin_data,x_valid )\n",
        "  y_train=np.setdiff1d(y_origin_data,y_valid )\n",
        "\n",
        "  if len(x_train.shape)==1 and len(y_train.shape)==1:\n",
        "    x_train = np.expand_dims(x_train, axis=-1)\n",
        "    y_train = np.expand_dims(y_train, axis=-1)\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "  for epoch in range(num_epochs):\n",
        "      # Convert numpy arrays to torch tensors\n",
        "      inputs = torch.from_numpy(x_train)\n",
        "      targets = torch.from_numpy(y_train)\n",
        "\n",
        "      # Predict outputs with the linear model.\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets)\n",
        "      \n",
        "      # compute gradients and update\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      if (epoch+1) % 20 == 0:\n",
        "          print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "  # validation error\n",
        "  inputs = torch.from_numpy(x_valid)\n",
        "  targets = torch.from_numpy(y_valid)\n",
        "  outputs = model(inputs)\n",
        "  loss = criterion(outputs, targets)\n",
        "  print(i+1,\"-th round validation error: \",loss.item())\n",
        "  val_losses.append(loss.item())\n",
        "\n",
        "val_losses = np.asarray(val_losses)\n",
        "print(\"Final validation error: \", val_losses.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZocMbDRxytuz",
        "outputId": "0ac28e6a-2928-46da-ebca-ce69a888fb20"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/100], Loss: 0.0232\n",
            "Epoch [40/100], Loss: 0.0024\n",
            "Epoch [60/100], Loss: 0.0003\n",
            "Epoch [80/100], Loss: 0.0000\n",
            "Epoch [100/100], Loss: 0.0000\n",
            "1 -th round validation error:  0.00017517681408207864\n",
            "Epoch [20/100], Loss: 0.0020\n",
            "Epoch [40/100], Loss: 0.0002\n",
            "Epoch [60/100], Loss: 0.0000\n",
            "Epoch [80/100], Loss: 0.0000\n",
            "Epoch [100/100], Loss: 0.0000\n",
            "2 -th round validation error:  3.7420144508359954e-05\n",
            "Epoch [20/100], Loss: 0.0015\n",
            "Epoch [40/100], Loss: 0.0002\n",
            "Epoch [60/100], Loss: 0.0000\n",
            "Epoch [80/100], Loss: 0.0000\n",
            "Epoch [100/100], Loss: 0.0000\n",
            "3 -th round validation error:  0.00010497155744815245\n",
            "Epoch [20/100], Loss: 0.0029\n",
            "Epoch [40/100], Loss: 0.0003\n",
            "Epoch [60/100], Loss: 0.0000\n",
            "Epoch [80/100], Loss: 0.0000\n",
            "Epoch [100/100], Loss: 0.0000\n",
            "4 -th round validation error:  4.4390086259227246e-05\n",
            "Epoch [20/100], Loss: 0.0009\n",
            "Epoch [40/100], Loss: 0.0001\n",
            "Epoch [60/100], Loss: 0.0000\n",
            "Epoch [80/100], Loss: 0.0000\n",
            "Epoch [100/100], Loss: 0.0000\n",
            "5 -th round validation error:  0.00016486275126226246\n",
            "Final validation error:  0.00010536427071201615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 결론 : 테스트셋이 많을수록 성능이 좋다"
      ],
      "metadata": {
        "id": "n0Z-v2ltzHau"
      }
    }
  ]
}